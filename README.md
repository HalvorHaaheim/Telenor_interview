## Spike Protection

You are working as a developer for a company that delivers security monitoring for Enterprise customers. Your solution is ingesting alerts that have been generated by a third-party tool.

### The task

Alerts are constantly being generated and placed onto a queue for you to process. On regular intervals all events currently in the queue are processed. Based on detected spikes, some events are discarded entirely, while the rest are removed from the queue and passed along for further processing (leaving your queue empty). These alerts have a few notable properties; `rule_name`, `customer_id` and `env`. 

These are the scenarios that we want to detect and handle:

1. An excessive amount of alerts are being generated for one or more rules for a given customer on a given environment (local spike)
2. An excessive amount of alerts are being generated for one or more rules across all customers on a given environment (global spike)

Create a solution that processes the information on the queue to determine if there are any ongoing spikes. We want to know which events should be kept, and which events should be removed. Choose the thresholds for local and global spikes yourself, and provide a rationale for your choice.

You must use Redis to store the events in the format used by the utility scripts:

```
{ env: "A", rule_name: "Malicious Document Macro", customer_id: "9943", "event_id": "24e8aca7-d16b-4a07-92a8-c411aebcf814" }
```

You are free to modify the scripts as you see fit, add new functionality etc. You are not required to use Python for this task, and can use the language you feel most comfortable with. The only requirement is that we can add events to Redis and subsequently run your script to process them.

## Quick start:

### Setting up your environment (Debian)

1. Install python3-venv

`sudo apt install python3-venv`

2. Navigate to the project folder

`cd /path/to/interview-case/`

3. Create a virtual environment

`python3 -m venv .venv`

4. Activate your virtual environment

`source .venv/bin/activate`

5. Install poetry in your environment

`pip3 install poetry`

6. Install additional dependencies in your environment

`poetry install`

7. Verify your install

`poetry run python3 interview/verify.py`

8. Install `Docker` and `docker-compose`

Installation instructions can be found at: https://docs.docker.com/get-docker/

9. Start the Redis-server

`docker-compose up`

## Additional tools

Neither Redis Tools nor Go-Task is strictly required to run the project. These only provide utilities that can be used to speed up your workflow, using them is entirely optional.

In other words, running `go-task local-spike` is equivalent to running `poetry run python3 interview/local_spike.py`.

1. Install Redis Tools (Optional, can be used to inspect events on the Redis queue)

`sudo apt install redis-tools`

2. Install Go-Task (https://github.com/go-task/task) (Optional, can be used to run python files more quickly)

Follow the instructions at: https://taskfile.dev/installation/

## Included in this project:

1. `global_spike.py`. When run, this file will insert events onto the Redis queue such that a global spike is guaranteed. Note that there may also be additional local spikes generated. You can also use the taskfile command `go-task global-spike` to run this file.
2. `local_spike.py`. When run, this file will insert events onto the Redis queue such that a local spike is guaranteed. Note that there may also be additional global spikes generated. You can also use the taskfile command `go-task local-spike` to run this file.
3. `clear.py`. When run, this file will clear all events from Redis. You can also use the taskfile command `go-task clear` to run this file.
4. `verify.py`. Used to verify that your setup is working.
5. `Taskfile.yml`. This is a taskfile that can be used to quickly generate `global` spikes, `local` spikes, or to clear the queue entirely. Run commands using `go-task <COMMAND>`.

### Using Taskfile

1. `go-task global-spike` -> Create a `global` spike. The rule_name and environment is chosen randomly and printed after running the command.
2. `go-task local-spike` -> Create a `local` spike on a customer. The rule_name, customer and environment is chosen randomly and printed after running the command.
3. `go-task clear` -> Clear all events from the queue in Redis

### Checking Redis for Events

This is not strictly necessary, but can be useful to ensure you have events in Redis after running the `go-task` commands or python-scripts.

```bash
user@home $ redis-cli -p 5000

127.0.0.1:5000> auth super-secret
OK

llen events
115

lrange events 0 0
1) "8414075a-ca1b-41a9-841f-198f54669245"

127.0.0.1:5000> get 8414075a-ca1b-41a9-841f-198f54669245
"{\"event_id\": \"8414075a-ca1b-41a9-841f-198f54669245\", \"rule_name\": \"Process Stopped by Deletion\", \"customer_id\": \"4432\", \"env\": \"A\"}"
```

### Example 1:

Assume we have the following thresholds:

Local spike: 5

Global spike: 25

The queue looks like this:

```javascript
[
  { env: "A", rule_name: "Malicious Document Macro", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "B", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "C", rule_name: "Process stopped by deletion", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "1234" },
  { env: "B", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "B", rule_name: "SQL Injection 200 Response", customer_id: "9943" },
  { env: "B", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
];
```

There is a local spike on customer `1234` on environment `A`, because the rule `Suspicious Powershell Process` has triggered more than 5 times in quick succession on environment `A`. There is no global spike, because the number of alerts for any given rule does not exceed `25` across all customers.

### Example 2:

Assume we have the following thresholds:

Local spike: 5

Global spike: 10

The queue looks like this:

```javascript
[
  { env: "C", rule_name: "Process stopped by deletion", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "5321" },
  { env: "B", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "B", rule_name: "Powershell Execution Policy Bypass", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "A", rule_name: "SQL Injection 200 Response", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "B", rule_name: "Powershell Execution Policy Bypass", customer_id: "1234" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "C", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "C", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "1234" },
  { env: "D", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "D", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Malicious Document Macro", customer_id: "9943" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
  { env: "A", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "B", rule_name: "Suspicious Powershell Process", customer_id: "9943" },
  { env: "B", rule_name: "Suspicious Powershell Process", customer_id: "5321" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
  { env: "A", rule_name: "Powershell Execution Policy Bypass", customer_id: "9943" },
];
```

There is a global spike for the rule `Suspicious Powershell Process` on environment `A`, since it has triggered more than 10 times in quick succession across all customers on environment `A`.

There is a local spike ongoing for customer `9943` because the rule `Powershell Execution Policy Bypass` has triggered more than 5 times in quick succession on customer `9943` on environment `A`.

There is a local spike ongoing for customer `1234` because the rule `Suspicious Powershell Process` has triggered more than 5 times in quick succession on customer `1234` on environment `A`.
